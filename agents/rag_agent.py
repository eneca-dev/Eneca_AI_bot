"""RAG Agent for knowledge base search with Cohere Reranking"""
from pathlib import Path
from typing import List, Dict
from agents.base import BaseAgent
from core.vector_store import vector_store_manager
from core.reranker import reranker
from core.config import settings
from loguru import logger


class RAGAgent(BaseAgent):
    """RAG Agent that searches knowledge base and provides factual answers"""

    def __init__(self, model: str = None, temperature: float = None):
        """
        Initialize RAG agent

        Args:
            model: OpenAI model name (defaults to config value)
            temperature: Lower temperature for more factual responses (defaults to config value)
        """
        # Use config defaults if not specified
        model = model or settings.rag_agent_model
        temperature = temperature if temperature is not None else settings.rag_agent_temperature

        super().__init__(model=model, temperature=temperature)
        self.vector_store = vector_store_manager
        logger.info(f"RAGAgent initialized with model {model} and temperature {temperature}")

    def _get_default_prompt(self) -> str:
        """Load system prompt from prompts/rag_agent.md"""
        prompt_path = Path(__file__).parent.parent / "prompts" / "rag_agent.md"

        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt = f.read()
            logger.debug(f"Loaded RAG agent prompt from {prompt_path}")
            return prompt
        except FileNotFoundError:
            logger.warning(f"Prompt file not found at {prompt_path}, using default")
            return (
                "You are a RAG agent. Search the knowledge base and provide accurate, "
                "factual answers based only on the information found in the documents."
            )

    def search_knowledge_base(self, query: str, k: int = None) -> str:
        """
        Search knowledge base with optional Cohere reranking

        Pipeline:
        1. Vector search (get more candidates with lower threshold)
        2. Cohere rerank (if enabled) to select top-n most relevant
        3. Format results for LLM context

        Args:
            query: User's search query
            k: Number of final results to return (defaults to RERANK_TOP_N or VECTOR_SEARCH_K)

        Returns:
            Formatted string with search results
        """
        if not self.vector_store.is_available():
            logger.warning("Vector store not available")
            return (
                "Ğ‘Ğ°Ğ·Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°. "
                "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ĞµÑÑŒ Ğº Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ñƒ."
            )

        try:
            # Determine search parameters based on reranker availability
            if reranker.is_available():
                # Get more candidates for reranking (10 candidates -> top 3 after rerank)
                search_k = 10
                final_k = k or settings.rerank_top_n
                logger.info(f"Reranker enabled: fetching {search_k} candidates, will return top {final_k}")
            else:
                # No reranker - use standard search
                search_k = k or settings.vector_search_k
                final_k = search_k
                logger.info(f"Reranker disabled: fetching {search_k} documents directly")

            # Step 1: Vector search (get candidates)
            documents = self.vector_store.search_with_score(query=query, k=search_k)

            if not documents:
                return "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ²Ğ°ÑˆĞµĞ¼Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹."

            # Step 2: Rerank if available
            if reranker.is_available() and len(documents) > 0:
                documents = reranker.rerank(
                    query=query,
                    documents=documents,
                    top_n=final_k
                )
                logger.info(f"Reranked {search_k} -> {len(documents)} documents")
            else:
                # Limit to final_k without reranking
                documents = documents[:final_k]

            # Step 3: Format results
            result = "ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ:\n\n"
            for i, doc in enumerate(documents, 1):
                content = doc["content"]
                score = doc.get("score", 0)
                relevance = doc.get("relevance", "unknown")
                is_reranked = doc.get("reranked", False)

                score_label = f"rerank: {score:.3f}" if is_reranked else f"vector: {score:.2f}"
                result += f"[Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚ {i}] (Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ: {relevance}, {score_label})\n"
                result += f"{content}\n\n"

            return result.strip()

        except Exception as e:
            logger.error(f"Error searching knowledge base: {e}")
            return (
                "ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹. "
                "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ."
            )

    def answer_question(self, question: str) -> str:
        """
        Answer user question using RAG approach

        Args:
            question: User's question

        Returns:
            Answer based on knowledge base
        """
        logger.info(f"ğŸ“š RAG AGENT: processing question: '{question}'")

        # Search knowledge base
        context = self.search_knowledge_base(question)
        logger.info(f"ğŸ“š RAG AGENT: got context ({len(context)} chars)")

        # If no relevant documents found, return early
        if "Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°" in context or "Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°" in context or "Ğ¾ÑˆĞ¸Ğ±ĞºĞ°" in context.lower():
            logger.warning(f"ğŸ“š RAG AGENT: no relevant docs found, returning early")
            return context

        # Construct prompt with context
        prompt_with_context = f"""ĞĞ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ.

Ğ’ĞĞ–ĞĞ: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°. ĞĞµ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹ Ğ¸ Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ½Ğµ.

ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹:
{context}

Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {question}

Ğ¢Ğ²Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½:
1. Ğ‘Ñ‹Ñ‚ÑŒ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Ğ²Ñ‹ÑˆĞµ
2. Ğ‘Ñ‹Ñ‚ÑŒ Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¼ Ğ¸ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼
3. Ğ•ÑĞ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¿Ğ¾Ğ»Ğ½Ğ°Ñ â€” Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¾Ğ± ÑÑ‚Ğ¾Ğ¼ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ
4. Ğ‘Ñ‹Ñ‚ÑŒ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ
"""

        # Get answer from LLM
        try:
            response = self.invoke(prompt_with_context)
            logger.info("RAG Agent successfully generated answer")
            return response

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return (
                "ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑÑ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚. "
                "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿ĞµÑ€ĞµÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ."
            )

    def process_message(self, user_message: str) -> str:
        """
        Process user message (alias for answer_question)

        Args:
            user_message: User's input message

        Returns:
            Agent's response
        """
        return self.answer_question(user_message)
