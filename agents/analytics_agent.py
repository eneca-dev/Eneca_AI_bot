"""Analytics Agent for data analysis, reporting, and visualization"""
from pathlib import Path
from typing import Dict, Any, List, Optional
import json
import time
from loguru import logger
from tenacity import retry, stop_after_attempt, wait_exponential

from agents.base import BaseAgent
from database.supabase_client import supabase_db_client
from core.config import settings
from agents.sql_generator import SQLGenerator

# Import shared models to avoid circular import
from agents.analytics_models import FilterOptions, AnalyticsQuery, AnalyticsResult


class AnalyticsAgent(BaseAgent):
    """
    Analytics Agent for data analysis and reporting

    Capabilities:
    - SQL query generation and execution
    - Statistical analysis
    - Report generation
    - Chart data preparation (for frontend rendering)
    - Comparison and trend analysis
    """

    def __init__(self, model: str = None, temperature: float = None):
        """
        Initialize Analytics agent

        Args:
            model: OpenAI model name (defaults to config value)
            temperature: Lower for precise SQL, higher for creative insights
        """
        model = model or settings.orchestrator_model
        temperature = temperature if temperature is not None else 0.2  # Precise for SQL

        super().__init__(model=model, temperature=temperature)
        self.db = supabase_db_client

        # Configure LLM with structured output
        self.query_llm = self.llm.with_structured_output(AnalyticsQuery)

        # Initialize SQL Generator
        self.sql_generator = SQLGenerator()

        # Initialize circuit breaker for SQL execution protection
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60
        )

        logger.info(f"AnalyticsAgent initialized with model {model}")

    def _get_default_prompt(self) -> str:
        """Load system prompt from prompts/analytics_agent.md"""
        prompt_path = Path(__file__).parent.parent / "prompts" / "analytics_agent.md"

        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt = f.read()
            logger.debug(f"Loaded Analytics agent prompt from {prompt_path}")
            return prompt
        except FileNotFoundError:
            logger.warning(f"Prompt file not found at {prompt_path}, using default")
            return self._get_fallback_prompt()

    def _get_fallback_prompt(self) -> str:
        """Fallback prompt if file not found"""
        return """–¢—ã ‚Äî Analytics Agent –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞–º–∏ Eneca.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ, –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç—ã –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏–∫—É –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö:
- projects: –ø—Ä–æ–µ–∫—Ç—ã (id, name, status, created_at, updated_at)
- stages: —ç—Ç–∞–ø—ã –ø—Ä–æ–µ–∫—Ç–æ–≤ (id, project_id, name, start_date, end_date, progress)
- objects: –æ–±—ä–µ–∫—Ç—ã –≤ —ç—Ç–∞–ø–∞—Ö (id, stage_id, name, responsible_id, status)
- sections: —Ä–∞–∑–¥–µ–ª—ã –≤ –æ–±—ä–µ–∫—Ç–∞—Ö (id, object_id, name, progress)
- profiles: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (id, email, first_name, last_name, job_title, department)

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SQL –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
2. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
4. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤

–í–ê–ñ–ù–û:
- –í—Å–µ SQL –∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º–∏ (—Ç–æ–ª—å–∫–æ SELECT)
- –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- –í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
- –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (role-based)
"""

    def _parse_user_query(self, query: str, user_role: Optional[str] = None) -> AnalyticsQuery:
        """
        Parse natural language query into structured analytics query

        Args:
            query: User's natural language query
            user_role: User's role for access control

        Returns:
            Structured AnalyticsQuery
        """
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–π –µ–≥–æ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
–†–æ–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_role or 'guest'}

–û–ø—Ä–µ–¥–µ–ª–∏:
1. intent: —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ (report, chart, statistics, sql_query, comparison, complex_join)
   - complex_join: –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –ù–ï–°–ö–û–õ–¨–ö–ò–• —Ç–∞–±–ª–∏—Ü (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–æ–±—ä–µ–∫—Ç—ã —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏ –∏ –±—é–¥–∂–µ—Ç–æ–º", "–ø—Ä–æ–µ–∫—Ç—ã —Å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏")
   - report: –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –¥–ª—è –û–î–ù–û–ô —Ç–∞–±–ª–∏—Ü—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä: "—Å–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–æ–≤", "–º–æ–∏ —ç—Ç–∞–ø—ã")
2. entities: –∫–∞–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å (projects, stages, objects, sections, tasks, profiles, view_employee_workloads, v_budgets_full, view_project_dashboard)
   - –î–ª—è complex_join: —É–∫–∞–∂–∏ –í–°–ï –Ω—É–∂–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (–≥–ª–∞–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–µ—Ä–≤–æ–π)
3. metrics: –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω—É–∂–Ω—ã (count, sum, avg, progress, status_distribution, loading_rate, budget, spent, hours, first_name, last_name, email)
4. filters: –∫–∞–∫–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å (date_range, status, department, responsible, entity_type)
5. aggregation: –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ (daily, weekly, monthly, by_user, by_project)
6. chart_type: —Ç–∏–ø –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (bar, line, pie, table, mixed)
7. personalized: –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (true –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞ "–º–æ–π/–º–æ—è/–º–æ–∏/–º–Ω–µ", –∏–Ω–∞—á–µ false)

–ö–†–ò–¢–ò–ß–ù–´–ï –ü–†–ê–í–ò–õ–ê –î–õ–Ø –í–´–ë–û–†–ê ENTITY (—á–∏—Ç–∞–π –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û —Å–ª–æ–≤–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ):
- –ü—Ä–æ–µ–∫—Ç/–ø—Ä–æ–µ–∫—Ç—ã/–ø—Ä–æ–µ–∫—Ç–æ–≤ ‚Üí projects
- –≠—Ç–∞–ø/—ç—Ç–∞–ø—ã/—ç—Ç–∞–ø–æ–≤/—Å—Ç–∞–¥–∏—è/—Å—Ç–∞–¥–∏–∏ ‚Üí stages
- –û–±—ä–µ–∫—Ç/–æ–±—ä–µ–∫—Ç—ã/–æ–±—ä–µ–∫—Ç–æ–≤ ‚Üí objects
- –†–∞–∑–¥–µ–ª/—Ä–∞–∑–¥–µ–ª—ã/—Ä–∞–∑–¥–µ–ª–æ–≤/—Å–µ–∫—Ü–∏—è ‚Üí sections
- –ó–∞–¥–∞—á–∞/–∑–∞–¥–∞—á–∏/–∑–∞–¥–∞—á/—Ç–∞—Å–∫ ‚Üí tasks
- –°–æ—Ç—Ä—É–¥–Ω–∏–∫/—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å/—é–∑–µ—Ä/—á–µ–ª–æ–≤–µ–∫ ‚Üí profiles
- –ó–∞–≥—Ä—É–∑–∫–∞/–∑–∞–Ω—è—Ç–æ—Å—Ç—å/–ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω/–∫—Ç–æ –∑–∞–≥—Ä—É–∂–µ–Ω ‚Üí view_employee_workloads
- –ë—é–¥–∂–µ—Ç/—Ñ–∏–Ω–∞–Ω—Å—ã/–¥–µ–Ω—å–≥–∏/–ø–æ—Ç—Ä–∞—á–µ–Ω–æ/–æ—Å—Ç–∞—Ç–æ–∫/—Ä–∞—Å—Ö–æ–¥ ‚Üí v_budgets_full
- –ß–∞—Å—ã/–ø–ª–∞–Ω —á–∞—Å–æ–≤/—Ñ–∞–∫—Ç —á–∞—Å–æ–≤/—Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã ‚Üí view_project_dashboard

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ "—ç—Ç–∞–ø—ã" –∏–ª–∏ "—Å—Ç–∞–¥–∏–∏" - –ù–ï –í–´–ë–ò–†–ê–ô projects! –í—ã–±–∏—Ä–∞–π stages!
–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ "–æ–±—ä–µ–∫—Ç—ã" - –ù–ï –í–´–ë–ò–†–ê–ô projects! –í—ã–±–∏—Ä–∞–π objects!
–í–ê–ñ–ù–û: –ï—Å–ª–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ —è–≤–Ω–æ –Ω–∞–ø–∏—Å–∞–Ω–æ "—Ä–∞–∑–¥–µ–ª—ã" - –ù–ï –í–´–ë–ò–†–ê–ô projects! –í—ã–±–∏—Ä–∞–π sections!

–ö–†–ò–¢–ò–ß–ù–û: –î–ª—è entities –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ç–æ, —á—Ç–æ –Ø–í–ù–û —É–∫–∞–∑–∞–Ω–æ –≤ –∑–∞–ø—Ä–æ—Å–µ!
- "–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏" ‚Üí entities=["projects", "profiles"] (–ù–ï –¥–æ–±–∞–≤–ª—è–π v_budgets_full!)
- "–ü—Ä–æ–µ–∫—Ç—ã —Å –±—é–¥–∂–µ—Ç–æ–º" ‚Üí entities=["projects", "v_budgets_full"] (–ù–ï –¥–æ–±–∞–≤–ª—è–π profiles!)
- "–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã" ‚Üí entities=["projects"], filters={{"status": "active"}} (–ù–ï –¥–æ–±–∞–≤–ª—è–π profiles –∏–ª–∏ budgets!)
- "–ü—Ä–æ–µ–∫—Ç—ã" ‚Üí entities=["projects"] (–¢–û–õ–¨–ö–û –ø—Ä–æ–µ–∫—Ç—ã, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü!)

–ü—Ä–∏–º–µ—Ä—ã:
- "–ü–æ–∫–∞–∂–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º" ‚Üí entities=["projects"]
- "–ú–æ–∏ –ø—Ä–æ–µ–∫—Ç—ã" ‚Üí entities=["projects"], personalized=true
- "–ü–æ–∫–∞–∂–∏ —ç—Ç–∞–ø—ã" ‚Üí entities=["stages"]
- "–°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤" ‚Üí entities=["stages"]
- "–°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤" ‚Üí entities=["objects"]
- "–ú–æ–∏ –æ–±—ä–µ–∫—Ç—ã" ‚Üí entities=["objects"], personalized=true
- "–†–∞–∑–¥–µ–ª—ã –ø—Ä–æ–µ–∫—Ç–∞" ‚Üí entities=["sections"]
- "–ó–∞–¥–∞—á–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤" ‚Üí entities=["tasks"]
- "–ö—Ç–æ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω?" ‚Üí entities=["view_employee_workloads"]
- "–ë—é–¥–∂–µ—Ç –ø—Ä–æ–µ–∫—Ç–æ–≤" ‚Üí entities=["v_budgets_full"], filters={{"entity_type": "project"}}
- "–¢–æ–ø 5 –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ –±—é–¥–∂–µ—Ç—É" ‚Üí entities=["v_budgets_full"], chart_type="bar"
- "–°–∫–æ–ª—å–∫–æ –ø–æ—Ç—Ä–∞—á–µ–Ω–æ –¥–µ–Ω–µ–≥" ‚Üí entities=["v_budgets_full"], metrics=["spent"]
- "–û—Å—Ç–∞—Ç–æ–∫ –±—é–¥–∂–µ—Ç–∞" ‚Üí entities=["v_budgets_full"], metrics=["remaining"]
- "–ß–∞—Å—ã –ø–æ –ø—Ä–æ–µ–∫—Ç–∞–º" ‚Üí entities=["view_project_dashboard"]

–ü–†–ò–ú–ï–†–´ –û–î–ù–û–ô –¢–ê–ë–õ–ò–¶–´ (intent=report):
- "–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã" ‚Üí intent="report", entities=["projects"], filters={{"status": "active"}}
- "–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏" ‚Üí intent="complex_join", entities=["projects", "profiles"]
- "–í—Å–µ –ø—Ä–æ–µ–∫—Ç—ã" ‚Üí intent="report", entities=["projects"]
- "–°–ø–∏—Å–æ–∫ —ç—Ç–∞–ø–æ–≤" ‚Üí intent="report", entities=["stages"]
- "–ú–æ–∏ –æ–±—ä–µ–∫—Ç—ã" ‚Üí intent="report", entities=["objects"], personalized=true

–ü–†–ò–ú–ï–†–´ COMPLEX_JOIN (–¢–û–õ–¨–ö–û –µ—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—É—â–Ω–æ—Å—Ç–µ–π):
- "–û–±—ä–µ–∫—Ç—ã —Å –∏–º–µ–Ω–∞–º–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤" ‚Üí intent="complex_join", entities=["objects", "profiles"]
- "–ü—Ä–æ–µ–∫—Ç—ã —Å –±—é–¥–∂–µ—Ç–æ–º" ‚Üí intent="complex_join", entities=["projects", "v_budgets_full"]
- "–ü—Ä–æ–µ–∫—Ç—ã —Å –±—é–¥–∂–µ—Ç–æ–º –∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏" ‚Üí intent="complex_join", entities=["projects", "v_budgets_full", "profiles"]
- "–ó–∞–¥–∞—á–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞—Ö" ‚Üí intent="complex_join", entities=["tasks", "profiles"]
- "–≠—Ç–∞–ø—ã –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏" ‚Üí intent="complex_join", entities=["stages", "projects", "profiles"]

–ù–ï–ü–†–ê–í–ò–õ–¨–ù–´–ï –ø—Ä–∏–º–µ—Ä—ã (–ù–ï –¥–µ–ª–∞–π —Ç–∞–∫):
- "–ê–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã —Å –º–µ–Ω–µ–¥–∂–µ—Ä–∞–º–∏" ‚Üí ‚ùå entities=["projects", "profiles", "v_budgets_full"] (–ù–ï –¥–æ–±–∞–≤–ª—è–π –±—é–¥–∂–µ—Ç!)
- "–ü—Ä–æ–µ–∫—Ç—ã" ‚Üí ‚ùå entities=["projects", "profiles"] (–ù–ï –¥–æ–±–∞–≤–ª—è–π profiles –µ—Å–ª–∏ –Ω–µ —É–ø–æ–º—è–Ω—É—Ç—ã!)
"""

        try:
            parsed = self.query_llm.invoke(prompt)
            logger.info(f"Parsed query: {parsed}")
            return parsed
        except Exception as e:
            logger.error(f"Error parsing query: {e}")
            # Fallback: try to guess entity from query text
            query_lower = query.lower()
            entity = "projects"  # default

            # Check for keywords in order of specificity
            if any(word in query_lower for word in ['–∑–∞–≥—Ä—É–∑–∫', '–∑–∞–Ω—è—Ç–æ—Å—Ç', '–ø–µ—Ä–µ–≥—Ä—É–∂']):
                entity = "view_employee_workloads"
            elif any(word in query_lower for word in ['–±—é–¥–∂–µ—Ç', '—Ñ–∏–Ω–∞–Ω—Å', '–¥–µ–Ω—å–≥–∏', '–ø–æ—Ç—Ä–∞—á', '–æ—Å—Ç–∞—Ç–æ–∫', '—Ä–∞—Å—Ö–æ–¥']):
                entity = "v_budgets_full"
            elif any(word in query_lower for word in ['—á–∞—Å', '—Ç—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç']):
                entity = "view_project_dashboard"
            elif any(word in query_lower for word in ['—ç—Ç–∞–ø', '—Å—Ç–∞–¥–∏']):
                entity = "stages"
            elif any(word in query_lower for word in ['–æ–±—ä–µ–∫—Ç']):
                entity = "objects"
            elif any(word in query_lower for word in ['—Ä–∞–∑–¥–µ–ª', '—Å–µ–∫—Ü–∏']):
                entity = "sections"
            elif any(word in query_lower for word in ['–∑–∞–¥–∞—á', '—Ç–∞—Å–∫']):
                entity = "tasks"
            elif any(word in query_lower for word in ['—Å–æ—Ç—Ä—É–¥–Ω–∏–∫', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª', '—é–∑–µ—Ä', '—á–µ–ª–æ–≤–µ–∫']):
                entity = "profiles"
            elif any(word in query_lower for word in ['–ø—Ä–æ–µ–∫—Ç']):
                entity = "projects"

            logger.warning(f"Fallback entity selection: {entity}")

            return AnalyticsQuery(
                intent="report",
                entities=[entity],
                metrics=["count"]
            )

    def _generate_sql(
        self,
        parsed_query: AnalyticsQuery,
        user_role: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> str:
        """
        Generate SQL using SQLGenerator with RBAC

        Args:
            parsed_query: Structured analytics query
            user_role: User's role for RBAC filtering
            user_id: User's ID for personalized queries

        Returns:
            SQL query string
        """
        # Generate SQL with parameters
        sql, params = self.sql_generator.generate_sql(
            parsed_query,
            user_role or 'guest',
            user_id
        )

        # Inject parameters safely (escape SQL injection)
        sql = self.sql_generator._inject_parameters_safe(sql, params)

        logger.info(f"Generated SQL: {sql[:200]}...")
        return sql

    def _execute_sql(
        self,
        sql: str,
        user_role: str = 'guest'
    ) -> List[Dict[str, Any]]:
        """
        Execute SQL with retry logic and circuit breaker

        Args:
            sql: SQL query (SELECT only)
            user_role: User role for sensitive column filtering

        Returns:
            Query results as list of dicts
        """
        # Security check
        if not sql.strip().upper().startswith("SELECT"):
            raise ValueError("Only SELECT queries are allowed")

        logger.info(f"Executing SQL for role={user_role}")

        try:
            # Execute with retry
            data = self._execute_sql_with_retry(sql, user_role)

            # Filter sensitive columns based on role
            data = self._filter_sensitive_columns(data, user_role)

            return data

        except Exception as e:
            logger.error(f"SQL execution failed after retries: {e}")
            return []

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10)
    )
    def _execute_sql_with_retry(
        self,
        sql: str,
        user_role: str
    ) -> List[Dict[str, Any]]:
        """
        Execute SQL with retry and circuit breaker

        Args:
            sql: SQL query string
            user_role: User role for logging

        Returns:
            Query results

        Raises:
            Exception: If circuit breaker is open or execution fails
        """
        # Check circuit breaker
        if self.circuit_breaker.is_open():
            logger.warning("Circuit breaker OPEN - skipping SQL execution")
            raise Exception("Circuit breaker open")

        # Check if Supabase client is available
        if not self.db.is_available():
            logger.error("Supabase client not available")
            raise Exception("Supabase client not initialized")

        try:
            # Call RPC function
            response = self.db.client.rpc(
                'execute_analytics_query',
                {'query_text': sql, 'user_role_name': user_role}
            ).execute()

            # Record success
            self.circuit_breaker.record_success()

            # Parse JSONB result
            return self._parse_jsonb_result(response.data)

        except Exception as e:
            # Record failure
            self.circuit_breaker.record_failure()
            logger.error(f"RPC execution failed: {e}")
            raise

    def _parse_jsonb_result(self, data: List[Dict]) -> List[Dict]:
        """
        Parse JSONB from RPC response

        Args:
            data: Raw RPC response data

        Returns:
            Parsed list of dicts
        """
        if not data:
            return []

        # RPC returns [{"result": {...}}, {"result": {...}}, ...]
        if isinstance(data, list) and len(data) > 0:
            if 'result' in data[0]:
                return [row['result'] for row in data]

        return data

    def _filter_sensitive_columns(
        self,
        data: List[Dict[str, Any]],
        user_role: str
    ) -> List[Dict[str, Any]]:
        """
        Remove sensitive columns based on user role

        Args:
            data: Query results
            user_role: User's role

        Returns:
            Filtered data
        """
        if user_role in ['admin', 'manager']:
            # Full access
            return data

        # Define sensitive columns by role
        sensitive = {
            'guest': ['email', 'phone', 'password', 'first_name', 'last_name'],
            'viewer': ['email', 'phone', 'password'],
            'engineer': ['password']
        }

        blocked = sensitive.get(user_role, [])

        if not blocked:
            return data

        # Filter out sensitive columns
        return [
            {k: ('[Hidden]' if k in blocked else v) for k, v in row.items()}
            for row in data
        ]

    def _generate_empty_message(self, user_query: str, entity: str, personalized: bool) -> str:
        """
        Generate context-aware message when no data found

        Args:
            user_query: Original query
            entity: Entity type (projects, tasks, etc.)
            personalized: Whether query was personalized

        Returns:
            User-friendly message explaining why no data
        """
        entity_names = {
            'projects': '–ø—Ä–æ–µ–∫—Ç–æ–≤',
            'stages': '—ç—Ç–∞–ø–æ–≤',
            'objects': '–æ–±—ä–µ–∫—Ç–æ–≤',
            'sections': '—Ä–∞–∑–¥–µ–ª–æ–≤',
            'tasks': '–∑–∞–¥–∞—á',
            'profiles': '—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤',
            'view_employee_workloads': '–¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–≥—Ä—É–∑–∫–µ',
            'v_budgets_full': '–¥–∞–Ω–Ω—ã—Ö –æ –±—é–¥–∂–µ—Ç–µ',
            'view_project_dashboard': '–¥–∞–Ω–Ω—ã—Ö –æ —á–∞—Å–∞—Ö',
            'view_planning_analytics_summary': '–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö',
            'view_my_work_analytics': '–¥–∞–Ω–Ω—ã—Ö –æ –≤–∞—à–µ–π —Ä–∞–±–æ—Ç–µ'
        }

        entity_name = entity_names.get(entity, '–¥–∞–Ω–Ω—ã—Ö')

        if personalized:
            if entity == 'projects':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω—ã –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ."
            elif entity == 'tasks':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –≤–∞–º –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω—ã –∑–∞–¥–∞—á–∏."
            elif entity == 'objects':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–º –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ."
            elif entity == 'sections':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –Ω–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∏ –Ω–∞ –æ–¥–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ."
            elif entity == 'stages':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ—Ç —ç—Ç–∞–ø–æ–≤ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏, –≥–¥–µ –≤—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π."
            else:
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞—à–∏—Ö {entity_name}."
        else:
            if entity == 'view_employee_workloads':
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ {entity_name}. –í–æ–∑–º–æ–∂–Ω–æ, –¥–∞–Ω–Ω—ã–µ –æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –µ—â–µ –Ω–µ –≤–Ω–µ—Å–µ–Ω—ã."
            else:
                return f"–ü–æ –∑–∞–ø—Ä–æ—Å—É '{user_query}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ {entity_name}."

    def _is_data_empty(self, data: List[Dict[str, Any]]) -> bool:
        """
        Check if data contains only None values (empty view)

        Args:
            data: Query results

        Returns:
            True if all non-id values are None
        """
        if not data:
            return True

        # Check if all values (except IDs) are None
        for row in data:
            # Get non-id values
            values = [v for k, v in row.items() if 'id' not in k.lower() and 'name' not in k.lower()]
            # If any non-None value exists, data is not empty
            if any(v is not None for v in values):
                return False

        return True

    def _prepare_table_data(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Prepare table data in frontend-friendly format

        Args:
            data: Query results as list of dicts

        Returns:
            Dict with columns and rows structure
        """
        if not data:
            return {"columns": [], "rows": []}

        # Extract column names from first row, filter out ID columns
        all_columns = list(data[0].keys())
        columns = [
            col for col in all_columns
            if not (col.endswith('_id') or col == 'id')
        ]

        # Convert list of dicts to list of lists (only non-ID columns)
        rows = [[row.get(col) for col in columns] for row in data]

        return {
            "columns": columns,
            "rows": rows
        }

    def _prepare_chart_data(
        self,
        data: List[Dict[str, Any]],
        chart_type: str
    ) -> Dict[str, Any]:
        """
        Prepare chart configuration for frontend (Chart.js)

        Args:
            data: Query results
            chart_type: Type of chart

        Returns:
            Chart.js config
        """
        if chart_type == "pie":
            return {
                "type": "pie",
                "data": {
                    "labels": [row.get("label", row.get("name", "Unknown")) for row in data],
                    "datasets": [{
                        "data": [row.get("value", row.get("count", 0)) for row in data],
                        "backgroundColor": [
                            "#FF6384", "#36A2EB", "#FFCE56", "#4BC0C0", "#9966FF"
                        ]
                    }]
                },
                "options": {
                    "responsive": True,
                    "plugins": {
                        "legend": {"position": "bottom"}
                    }
                }
            }
        elif chart_type == "bar":
            return {
                "type": "bar",
                "data": {
                    "labels": [row.get("label", row.get("name", "")) for row in data],
                    "datasets": [{
                        "label": "–ó–Ω–∞—á–µ–Ω–∏–µ",
                        "data": [row.get("value", row.get("count", 0)) for row in data],
                        "backgroundColor": "#36A2EB"
                    }]
                },
                "options": {
                    "responsive": True,
                    "scales": {
                        "y": {"beginAtZero": True}
                    }
                }
            }
        elif chart_type == "line":
            return {
                "type": "line",
                "data": {
                    "labels": [row.get("date", row.get("label", "")) for row in data],
                    "datasets": [{
                        "label": "–î–∏–Ω–∞–º–∏–∫–∞",
                        "data": [row.get("value", 0) for row in data],
                        "borderColor": "#36A2EB",
                        "fill": False
                    }]
                },
                "options": {
                    "responsive": True,
                    "scales": {
                        "y": {"beginAtZero": True}
                    }
                }
            }
        else:
            return {}

    def process_analytics(
        self,
        user_query: str,
        user_role: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> AnalyticsResult:
        """
        Main analytics processing pipeline with RBAC support

        Args:
            user_query: Natural language query
            user_role: User role for RBAC filtering
            user_id: User ID for personalized queries

        Returns:
            AnalyticsResult with data and visualization config
        """
        logger.info(f"üìä ANALYTICS: query='{user_query}', role={user_role}, user_id={user_id}")

        try:
            # Step 1: Parse natural language to structured query
            parsed_query = self._parse_user_query(user_query, user_role)

            # Step 2: Generate SQL with RBAC
            sql = self._generate_sql(parsed_query, user_role, user_id)

            # Step 3: Execute SQL with retry
            data = self._execute_sql(sql, user_role or 'guest')

            # Check if data is empty or contains only None values
            if not data or self._is_data_empty(data):
                # Generate context-aware empty message
                entity = parsed_query.entities[0] if parsed_query.entities else '–ø—Ä–æ–µ–∫—Ç—ã'
                empty_message = self._generate_empty_message(user_query, entity, parsed_query.personalized)

                return AnalyticsResult(
                    type="text",
                    content=empty_message,
                    sql_query=sql,
                    metadata={"row_count": 0, "empty_data": True}
                )

            # Step 4: Determine result type
            # Special handling for workload queries - return analytics, not table
            if parsed_query.entities and 'view_employee_workloads' in parsed_query.entities:
                # Return text analysis with insights
                summary = self._generate_workload_analysis(data, user_query)
                return AnalyticsResult(
                    type="text",
                    content=summary,
                    sql_query=sql,
                    metadata={"row_count": len(data)}
                )
            elif parsed_query.chart_type and parsed_query.chart_type != 'table':
                # Return chart data (pie, bar, line, mixed)
                chart_config = self._prepare_chart_data(data, parsed_query.chart_type)
                return AnalyticsResult(
                    type="chart",
                    content=data,
                    sql_query=sql,
                    chart_config=chart_config,
                    metadata={"row_count": len(data)}
                )
            elif parsed_query.intent == "statistics":
                # Return text summary
                summary = self._generate_summary(data, parsed_query)
                return AnalyticsResult(
                    type="text",
                    content=summary,
                    sql_query=sql,
                    metadata={"row_count": len(data)}
                )
            else:
                # Return table (including when chart_type='table')
                table_data = self._prepare_table_data(data)
                return AnalyticsResult(
                    type="table",
                    content=table_data,
                    sql_query=sql,
                    metadata={"row_count": len(data)}
                )

        except Exception as e:
            logger.error(f"Analytics processing error: {e}")
            return AnalyticsResult(
                type="text",
                content=f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}",
                metadata={"error": str(e)}
            )

    def _generate_workload_analysis(self, data: List[Dict[str, Any]], user_query: str) -> str:
        """
        Generate workload analysis with insights

        Args:
            data: Workload data from view_employee_workloads
            user_query: Original user query

        Returns:
            Analytical text summary in Russian
        """
        if not data:
            return "–î–∞–Ω–Ω—ã—Ö –æ –∑–∞–≥—Ä—É–∑–∫–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        # Filter out rows with None loading_rate
        workload_data = [row for row in data if row.get('loading_rate') is not None]

        if not workload_data:
            return "–í –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —É —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –¥–∞–Ω–Ω—ã–µ –æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –µ—â–µ –Ω–µ –≤–Ω–µ—Å–µ–Ω—ã."

        # Analyze workload
        loading_rates = [row['loading_rate'] for row in workload_data]
        avg_load = sum(loading_rates) / len(loading_rates)

        overloaded = [row for row in workload_data if row['loading_rate'] > 100]
        high_load = [row for row in workload_data if 80 <= row['loading_rate'] <= 100]
        normal_load = [row for row in workload_data if 50 <= row['loading_rate'] < 80]
        low_load = [row for row in workload_data if row['loading_rate'] < 50]

        # Build analysis
        analysis = f"üìä **–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤**\n\n"
        analysis += f"–í—Å–µ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ —Å –∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π: {len(workload_data)}\n"
        analysis += f"–°—Ä–µ–¥–Ω—è—è –∑–∞–≥—Ä—É–∑–∫–∞: {avg_load:.1f}%\n\n"

        if overloaded:
            analysis += f"‚ö†Ô∏è **–ü–µ—Ä–µ–≥—Ä—É–∂–µ–Ω—ã ({len(overloaded)} —á–µ–ª.):**\n"
            for row in overloaded[:5]:  # Top 5
                analysis += f"- {row['full_name']}: {row['loading_rate']}% ({row.get('project_name', 'N/A')})\n"
            if len(overloaded) > 5:
                analysis += f"... –∏ –µ—â–µ {len(overloaded) - 5} —á–µ–ª–æ–≤–µ–∫\n"
            analysis += "\n"

        if high_load:
            analysis += f"üî∂ **–í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ({len(high_load)} —á–µ–ª.):** 80-100%\n\n"

        if normal_load:
            analysis += f"‚úÖ **–ù–æ—Ä–º–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ({len(normal_load)} —á–µ–ª.):** 50-80%\n\n"

        if low_load:
            analysis += f"üìâ **–ù–∏–∑–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ({len(low_load)} —á–µ–ª.):** <50%\n\n"

        # Recommendations
        if overloaded:
            analysis += "üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n"
            analysis += "- –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∑–∞–¥–∞—á–∏ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤\n"
            analysis += "- –ü—Ä–∏–≤–ª–µ—á—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∫ –ø—Ä–æ–µ–∫—Ç–∞–º\n"

        return analysis

    def _generate_summary(self, data: List[Dict[str, Any]], query: AnalyticsQuery) -> str:
        """Generate text summary from data"""
        if not data:
            return "–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

        # Use LLM to generate natural language summary
        data_str = json.dumps(data, ensure_ascii=False, indent=2)
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:

–ó–∞–ø—Ä–æ—Å: {query.intent} –ø–æ {', '.join(query.entities)}
–î–∞–Ω–Ω—ã–µ:
{data_str}

–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å –∫–ª—é—á–µ–≤—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏."""

        try:
            summary = self.invoke(prompt)
            return summary
        except Exception as e:
            logger.error(f"Summary generation error: {e}")
            return f"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}"

    def answer_question(self, question: str, user_role: Optional[str] = None) -> str:
        """
        Process analytics question (for orchestrator compatibility)

        Args:
            question: User question
            user_role: User role for RBAC

        Returns:
            Answer as string (may include JSON for structured data)
        """
        result = self.process_analytics(question, user_role)

        # Return as JSON for orchestrator to parse
        return json.dumps(result.model_dump(), ensure_ascii=False, indent=2)

    def process_message(self, user_message: str, user_role: Optional[str] = None) -> str:
        """Alias for answer_question"""
        return self.answer_question(user_message, user_role)


class CircuitBreaker:
    """
    Circuit breaker pattern for SQL execution protection

    Prevents cascading failures by stopping requests when error rate is too high.
    States: closed (normal), open (blocking), half_open (testing recovery)
    """

    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        """
        Initialize circuit breaker

        Args:
            failure_threshold: Number of failures before opening circuit
            recovery_timeout: Seconds to wait before attempting recovery
        """
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'closed'  # closed, open, half_open

    def is_open(self) -> bool:
        """
        Check if circuit is open (blocking requests)

        Returns:
            True if circuit is open
        """
        if self.state == 'open':
            # Try to recover after timeout
            if time.time() - self.last_failure_time > self.recovery_timeout:
                logger.info("Circuit breaker: transitioning to HALF-OPEN")
                self.state = 'half_open'
                return False
            return True
        return False

    def record_success(self):
        """Record successful execution - reset failure count"""
        if self.state == 'half_open':
            logger.info("Circuit breaker: CLOSED (recovered)")
        self.state = 'closed'
        self.failure_count = 0

    def record_failure(self):
        """Record failed execution - increment failure count"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            logger.error(
                f"Circuit breaker: OPEN (threshold reached: {self.failure_count})"
            )
            self.state = 'open'
